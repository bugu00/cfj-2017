{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's scrape some inmate data\n",
    "\n",
    "Our goal in this exercise is to scrape the [roster of inmates in the Hennepin County Jail](https://www4.co.hennepin.mn.us/webbooking/search.asp) into a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Can we get everyone?\n",
    "\n",
    "What happens when we click the search box without entering a first or last name? We're directed to a page with the listing of the entire roster at a new URL.\n",
    "\n",
    "This is good news (some forms are set up to require a minimum number of characters). Now we need to check to see whether you can just _go_ to that URL without visiting the landing page first and clicking through -- in other words, does that page depend on a [cookie](https://en.wikipedia.org/wiki/HTTP_cookie) being passed?\n",
    "\n",
    "To test this, I usually open another browser window in incognito mode and paste in the URL. Success! Going to [https://www4.co.hennepin.mn.us/webbooking/resultbyname.asp](https://www4.co.hennepin.mn.us/webbooking/resultbyname.asp) dumps out a list of inmates, so that's where we'll start. You can also open your network tab and see what information is getting exchanged during the request.\n",
    "\n",
    "(For more complex dynamically created pages that rely on cookies, we'd probably need the `requests` [Session object](http://docs.python-requests.org/en/master/user/advanced/#session-objects).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Check out the inmate detail page\n",
    "\n",
    "Let's click on an inmate link. We want to look at two things:\n",
    "\n",
    "- Does each inmate have a unique URL with a consistent pattern? (Yes)\n",
    "- What information on the page do we want to collect? (Let's grab custody info, housing location, booking date/time and arresting agency)\n",
    "\n",
    "What's the pattern for an inmate URL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Start scraping\n",
    "\n",
    "#### Import the libraries we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set introductory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base URL\n",
    "url_base = 'https://www4.co.hennepin.mn.us/webbooking/'\n",
    "\n",
    "# results page URL\n",
    "results_page = url_base + 'resultbyname.asp'\n",
    "\n",
    "# pattern for inmate detail URLs\n",
    "inmate_url_pattern = url_base + 'chargedetail.asp?v_booknum={}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch and parse the page contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fetch the page\n",
    "r = requests.get(results_page)\n",
    "\n",
    "# parse it\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# find the table we want\n",
    "table = soup.find_all('table')[6]\n",
    "\n",
    "# get the rows of the table, minus the header\n",
    "inmates = table.find_all('tr')[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a couple of functions\n",
    "\n",
    "We need to pause here and write a couple of functions to help us extract the bits of data from the inmate's detail page:\n",
    "\n",
    "- A function that takes the URL for an inmate detail page, fetches and parses the contents, then returns the bits of data we're interested in\n",
    "- A more specific function that takes the text of a label cell on a detail page (\"Sheriff's Custody:\", for instance) and returns the associated value in the next cell. This function will be called inside our other function -- it's not 100% necessary but it keeps us from repeating ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inmate_attr(soup, label):\n",
    "    \"\"\"Given a label and a soup'd detail page, return the associated value.\"\"\"\n",
    "    return soup.find(string=label).parent.parent.next_sibling \\\n",
    "                                  .next_sibling.text.strip()\n",
    "\n",
    "\n",
    "def inmate_details(url):\n",
    "    \"\"\"Fetch and parse and inmate detail page, return three bits of data.\"\"\"\n",
    "    \n",
    "    # fetch the page\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # parse it into soup\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    # call the get_inmate_attr function to nab the cells we're interested in\n",
    "    custody = get_inmate_attr(soup, \"Sheriff's Custody:\")\n",
    "    housing = get_inmate_attr(soup, \"Housing Location:\")\n",
    "    booking_date = get_inmate_attr(soup, \"Received Date/Time:\")\n",
    "\n",
    "    # return a dict with this info\n",
    "    # lose the \" Address\" string on the housing cell, where it exists\n",
    "    # also, parse the booking date as a date to validate\n",
    "    return {\n",
    "        'custody': custody,\n",
    "        'housing': housing.replace(' Address', ''),\n",
    "        'booking_date': datetime.strptime(booking_date, '%m/%d/%Y.. %H:%M')\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop over the inmate rows, write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open a file to write to\n",
    "with open('inmates.csv', 'w') as outfile:\n",
    "\n",
    "    # define your headers -- they should match the keys in the dict\n",
    "    # we're creating as we scrape\n",
    "    headers = ['booking_num', 'url', 'last', 'rest', 'dob',\n",
    "               'custody', 'housing', 'booking_date']\n",
    "\n",
    "    # create a writer object\n",
    "    writer = csv.DictWriter(outfile, fieldnames=headers)\n",
    "\n",
    "    # write the header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # print some summary info\n",
    "    print('')\n",
    "    print('Writing data for {:,} inmates ...'.format(len(inmates)))\n",
    "    print('')\n",
    "\n",
    "    # loop over the rows of inmates from the search results page\n",
    "    for row in inmates:\n",
    "        \n",
    "        # unpack the list of cells in the row\n",
    "        booking_num, name, dob, status = row.find_all('td')\n",
    "        \n",
    "        # get the detail page link using the template string we defined up top\n",
    "        detail_link = inmate_url_pattern.format(booking_num.string)\n",
    "        \n",
    "        # unpack the name into last/rest and print it\n",
    "        last, rest = name.string.split(', ')\n",
    "        print(rest, last)\n",
    "\n",
    "        # reformat the dob, which, bonus, also validates it\n",
    "        dob_parsed = datetime.strptime(dob.string, '%m/%d/%Y')\n",
    "\n",
    "        # our dict of summary info\n",
    "        summary_info = {\n",
    "            'booking_num': booking_num.string,\n",
    "            'url': detail_link,\n",
    "            'last': last,\n",
    "            'rest': rest,\n",
    "            'dob': dob_parsed.strftime('%Y-%m-%d')\n",
    "        }\n",
    "\n",
    "        # call the inmate_details function on the detail URL\n",
    "        # remember: this returns a dictionary\n",
    "        details = inmate_details(detail_link)\n",
    "\n",
    "        # combine the summary and detail dicts\n",
    "        combined_dict = {\n",
    "            **summary_info,\n",
    "            **details\n",
    "        }\n",
    "\n",
    "        # write the combined dict out to file\n",
    "        writer.writerow(combined_dict)\n",
    "\n",
    "        # pause for 2 seconds to give the server a break\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Extra credit_: Get charge details\n",
    "\n",
    "It's all well and good to get the basic inmate info, but we're probably also interested in _why_ they're in jail -- what are they charged with?\n",
    "\n",
    "For this exercise, add some parsing logic to the `inmate_details` scraping function to extract data about what each inmate has been charged with. Pulling them out as a list of dictionaries makes the most sense to me, but you can format it however you like.\n",
    "\n",
    "Because each inmate has a variable number of charges, you also need to think about how you want to represent the data in your CSV. Is each line one charge? One inmate? Picture how one row of data should look in your output file and structure your parsing to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
